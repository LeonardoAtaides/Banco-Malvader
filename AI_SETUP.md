# ü§ñ Chat IA - Banco Malvader

Assistente virtual inteligente que responde d√∫vidas dos clientes **100% localmente** (sem enviar dados para fora).

---

## üöÄ Instala√ß√£o R√°pida (5 minutos)

### 1Ô∏è‚É£ Instalar Ollama

Abra o PowerShell e execute:

```powershell
winget install Ollama.Ollama
```

### 2Ô∏è‚É£ Baixar o Modelo de IA

```powershell
ollama pull llama3.2:1b
```

> ‚è±Ô∏è **Aguarde 2-3 minutos** - Download de ~1GB

### 3Ô∏è‚É£ Configurar Banco de Dados

```powershell
npx prisma migrate dev
npx prisma generate
```

### 4Ô∏è‚É£ Pronto! Testar

```powershell
npm run dev
```

Acesse: http://localhost:3000/Cliente/Menu e clique no bot√£o üí¨ no canto inferior direito.

---

## ‚ùì Problemas Comuns

### ‚ö†Ô∏è Erro: "Model requires more system memory"

**Causa**: Pouca RAM dispon√≠vel (modelo precisa de ~4GB)

**Solu√ß√£o**:
1. Feche Chrome/Edge e outros programas pesados
2. Reinicie o computador
3. Tente novamente

Se ainda der erro, force uso de CPU apenas:
```powershell
# Feche o Ollama atual (√≠cone da bandeja)
# Abra PowerShell como Administrador:
$env:OLLAMA_NUM_GPU = "0"
ollama serve
```

---

### üêå Chat muito lento

**Normal!** Primeira mensagem demora 10-30 segundos (modelo carregando na mem√≥ria).

Pr√≥ximas mensagens: 3-5 segundos.

**Dica**: Use modelo maior se tiver 8GB+ RAM:
```powershell
ollama pull qwen2.5:3b
```

Depois altere em `.env`:
```
OLLAMA_MODEL="qwen2.5:3b"
```

---

## üîê Funcionalidades

‚úÖ **Privacidade**: Roda 100% local (nada enviado para internet)  
‚úÖ **Seguran√ßa**: Detecta e bloqueia dados sens√≠veis (CPF, senhas)  
‚úÖ **Hist√≥rico**: Conversas salvas no banco de dados  
‚úÖ **Gr√°tis**: Sem API keys, sem custos

---

## üìä Modelos Dispon√≠veis

| Modelo | RAM Necess√°ria | Velocidade | Qualidade | Instalar |
|--------|----------------|------------|-----------|----------|
| **llama3.2:1b** | 4GB | R√°pido | ‚≠ê‚≠ê | `ollama pull llama3.2:1b` |
| qwen2.5:3b | 8GB | M√©dio | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull qwen2.5:3b` |
| mistral:7b | 16GB | Lento | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull mistral:7b` |

> üí° **Padr√£o**: llama3.2:1b (funciona em qualquer PC b√°sico)

---

## ÔøΩÔ∏è Como Funciona (T√©cnico)

```
Usu√°rio digita mensagem
    ‚Üì
src/components/ai-chat.tsx (Frontend)
    ‚Üì
POST /api/ai/chat (Backend)
    ‚Üì
src/lib/ai/client.ts (Wrapper Ollama)
    ‚Üì
Ollama Local (http://localhost:11434)
    ‚Üì
Resposta da IA
    ‚Üì
Salvo no banco (prisma: chat_session/chat_message)
```

**Arquivos principais**:
- `src/components/ai-chat.tsx` - Interface do chat
- `src/app/api/ai/chat/route.ts` - API endpoint
- `src/lib/ai/client.ts` - Cliente Ollama
- `prisma/schema.prisma` - Tabelas: chat_session, chat_message

---

## üìù Uso no C√≥digo

```tsx
import AIChat from "@/components/ai-chat";

// Em qualquer p√°gina:
const [showChat, setShowChat] = useState(false);
const token = "seu-jwt-token"; // Pegar do login

return (
  <>
    <button onClick={() => setShowChat(true)}>üí¨ Assistente</button>
    {showChat && <AIChat token={token} onClose={() => setShowChat(false)} />}
  </>
);
```

---

## ÔøΩ Atualizar Modelo

```powershell
# Ver modelos instalados
ollama list

# Remover modelo antigo
ollama rm llama3.2:3b

# Instalar modelo novo
ollama pull qwen2.5:3b
```

Depois altere `.env`:
```
OLLAMA_MODEL="qwen2.5:3b"
```

---

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas (Opcional)

Edite `.env`:

```bash
# URL do Ollama (padr√£o: localhost)
OLLAMA_BASE_URL="http://localhost:11434"

# Modelo a usar
OLLAMA_MODEL="llama3.2:1b"
```

Para limitar uso de mem√≥ria, edite `src/lib/ai/client.ts`:

```typescript
options: {
  num_ctx: 2048,      // Contexto (menor = menos RAM)
  num_predict: 256,   // M√°x tokens resposta
  temperature: 0.7,   // Criatividade (0-1)
}
```

---

## üìû Suporte

- **Erro de instala√ß√£o**: Verifique se tem 4GB+ RAM livre
- **Ollama n√£o inicia**: Reinicie o PC (instala como servi√ßo Windows)
- **Chat n√£o aparece**: Verifique se est√° logado como cliente

---

**Licen√ßa**: Mesma do projeto principal
