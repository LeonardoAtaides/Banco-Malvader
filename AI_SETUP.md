# ü§ñ Chat IA - Banco Malvader

Assistente virtual inteligente que responde d√∫vidas dos clientes **100% localmente** (sem enviar dados para fora).

---

## üöÄ Instala√ß√£o R√°pida (5 minutos)

### 1Ô∏è‚É£ Instalar Ollama

Abra o PowerShell e execute:

```powershell
winget install Ollama.Ollama
```

### 2Ô∏è‚É£ Baixar o Modelo de IA

**Para PCs com pouca RAM (4-8GB):**
```powershell
ollama pull tinyllama
```
> ‚è±Ô∏è **Aguarde 1-2 minutos** - Download de ~637MB

**Para PCs com mais RAM (8GB+):**
```powershell
ollama pull llama3.2:1b
```
> ‚è±Ô∏è **Aguarde 2-3 minutos** - Download de ~1.3GB (melhor qualidade)

### 3Ô∏è‚É£ Configurar Banco de Dados

```powershell
npx prisma migrate dev
npx prisma generate
```

### 4Ô∏è‚É£ Pronto! Testar

```powershell
npm run dev
```

Acesse: http://localhost:3000/Cliente/Menu e clique no bot√£o üí¨ no canto inferior direito.

---

## ‚ùì Problemas Comuns

### ‚ö†Ô∏è Erro: "Model requires more system memory"

**Causa**: Pouca RAM dispon√≠vel

**Solu√ß√£o 1 - Usar modelo mais leve (RECOMENDADO):**
```powershell
ollama pull tinyllama
```

Depois altere em `.env`:
```
OLLAMA_MODEL="tinyllama"
```

Reinicie o servidor Next.js.

**Solu√ß√£o 2 - For√ßar modo CPU-only:**
```powershell
# Feche o Ollama atual (√≠cone da bandeja)
Stop-Process -Name ollama -Force -ErrorAction SilentlyContinue

# Configurar CPU-only permanentemente:
[System.Environment]::SetEnvironmentVariable('OLLAMA_NUM_GPU', '0', 'User')

# Reiniciar Ollama
Start-Process "ollama"
```

**Solu√ß√£o 3 - Liberar mem√≥ria:**
1. Feche Chrome/Edge e outros programas pesados
2. Reinicie o computador
3. Tente novamente

---

### üêå Chat muito lento

**Normal!** Primeira mensagem demora 10-30 segundos (modelo carregando na mem√≥ria).

Pr√≥ximas mensagens: 3-5 segundos.

---

## üìä Compara√ß√£o de Modelos

| Modelo | Tamanho | RAM Necess√°ria | Qualidade | Recomendado Para |
|--------|---------|----------------|-----------|------------------|
| `tinyllama` | 637MB | 1-1.5GB | ‚≠ê‚≠ê‚≠ê Razo√°vel | PCs com 4-6GB RAM |
| `llama3.2:1b` | 1.3GB | 2-3GB | ‚≠ê‚≠ê‚≠ê‚≠ê Boa | PCs com 8GB+ RAM |
| `phi3:mini` | 2.2GB | 3-4GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | PCs com 16GB+ RAM |

**Para trocar de modelo:**
```powershell
# Remover modelo atual
ollama rm tinyllama

# Instalar novo modelo
ollama pull llama3.2:1b

# Atualizar .env
# OLLAMA_MODEL="llama3.2:1b"
```

---

## üîê Funcionalidades

‚úÖ **Privacidade**: Roda 100% local (nada enviado para internet)  
‚úÖ **Seguran√ßa**: Detecta e bloqueia dados sens√≠veis (CPF, senhas)  
‚úÖ **Hist√≥rico**: Conversas salvas no banco de dados  
‚úÖ **Gr√°tis**: Sem API keys, sem custos

---

## üìä Modelos Dispon√≠veis

| Modelo | RAM Necess√°ria | Velocidade | Qualidade | Instalar |
|--------|----------------|------------|-----------|----------|
| **llama3.2:1b** | 4GB | R√°pido | ‚≠ê‚≠ê | `ollama pull llama3.2:1b` |
| qwen2.5:3b | 8GB | M√©dio | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull qwen2.5:3b` |
| mistral:7b | 16GB | Lento | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull mistral:7b` |

> üí° **Padr√£o**: llama3.2:1b (funciona em qualquer PC b√°sico)

---

## ÔøΩÔ∏è Como Funciona (T√©cnico)

```
Usu√°rio digita mensagem
    ‚Üì
src/components/ai-chat.tsx (Frontend)
    ‚Üì
POST /api/ai/chat (Backend)
    ‚Üì
src/lib/ai/client.ts (Wrapper Ollama)
    ‚Üì
Ollama Local (http://localhost:11434)
    ‚Üì
Resposta da IA
    ‚Üì
Salvo no banco (prisma: chat_session/chat_message)
```

**Arquivos principais**:
- `src/components/ai-chat.tsx` - Interface do chat
- `src/app/api/ai/chat/route.ts` - API endpoint
- `src/lib/ai/client.ts` - Cliente Ollama
- `prisma/schema.prisma` - Tabelas: chat_session, chat_message

---

## üìù Uso no C√≥digo

```tsx
import AIChat from "@/components/ai-chat";

// Em qualquer p√°gina:
const [showChat, setShowChat] = useState(false);
const token = "seu-jwt-token"; // Pegar do login

return (
  <>
    <button onClick={() => setShowChat(true)}>üí¨ Assistente</button>
    {showChat && <AIChat token={token} onClose={() => setShowChat(false)} />}
  </>
);
```

---

## ÔøΩ Atualizar Modelo

```powershell
# Ver modelos instalados
ollama list

# Remover modelo antigo
ollama rm llama3.2:3b

# Instalar modelo novo
ollama pull qwen2.5:3b
```

Depois altere `.env`:
```
OLLAMA_MODEL="qwen2.5:3b"
```

---

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas (Opcional)

Edite `.env`:

```bash
# URL do Ollama (padr√£o: localhost)
OLLAMA_BASE_URL="http://localhost:11434"

# Modelo a usar
OLLAMA_MODEL="llama3.2:1b"
```

Para limitar uso de mem√≥ria, edite `src/lib/ai/client.ts`:

```typescript
options: {
  num_ctx: 2048,      // Contexto (menor = menos RAM)
  num_predict: 256,   // M√°x tokens resposta
  temperature: 0.7,   // Criatividade (0-1)
}
```

---

## üìû Suporte

- **Erro de instala√ß√£o**: Verifique se tem 4GB+ RAM livre
- **Ollama n√£o inicia**: Reinicie o PC (instala como servi√ßo Windows)
- **Chat n√£o aparece**: Verifique se est√° logado como cliente

---

**Licen√ßa**: Mesma do projeto principal
